{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99550a2e-4cf4-4735-88a9-8e33b77ec970",
   "metadata": {},
   "source": [
    "# Reconocimiento de rostros con OpenCV, Python, and deep learning\n",
    "\n",
    "El reconocimiento facial con OpenCV, Python y deep learning es una técnica avanzada de visión artificial que permite a una computadora identificar y verificar rostros humanos en imágenes y videos. OpenCV es una biblioteca de procesamiento de imágenes y visión artificial que facilita la detección de rostros mediante técnicas de análisis de imágenes. Python es el lenguaje de programación utilizado para implementar y coordinar estos algoritmos, dado su versatilidad y la extensa disponibilidad de bibliotecas de deep learning. Finalmente, deep learning permite entrenar modelos basados en redes neuronales profundas que aprenden características faciales distintivas mediante \"aprendizaje de métrica profunda\" (deep metric learning), lo que permite clasificar y reconocer rostros de manera precisa y en tiempo real.\n",
    "\n",
    "OpenCV, que es como los ojos de la computadora. Python es el lenguaje que le habla a la computadora para decirle qué hacer. Y deep learning es como el cerebro de la computadora, que aprende y se entrena para reconocer caras, como cuando aprendes a recordar la cara de un amigo.\n",
    "\n",
    "Github: https://pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2900882",
   "metadata": {},
   "source": [
    "### Libreria dlib\n",
    "\n",
    "Es una libreria popular para machine learning y visión artificial, conocida por sus implementaciones de algoritmos de detección de rostros, alineación facial y otras técnicas de procesamiento de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcb3648-4916-4a5b-b777-5b8d9d5217ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Admin\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - dlib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.13.0               |   py39hcbf5309_1         1.0 MB  conda-forge\n",
      "    dlib-19.24.0               |   py39hf8509d4_0         4.4 MB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  dlib               conda-forge/win-64::dlib-19.24.0-py39hf8509d4_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.9-2_cp39\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-4.13.0-py39haa95532_0 --> conda-forge::conda-4.13.0-py39hcbf5309_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "conda-4.13.0         | 1.0 MB    |            |   0% \n",
      "conda-4.13.0         | 1.0 MB    | ####8      |  48% \n",
      "conda-4.13.0         | 1.0 MB    | ########## | 100% \n",
      "conda-4.13.0         | 1.0 MB    | ########## | 100% \n",
      "\n",
      "python_abi-3.9       | 4 KB      |            |   0% \n",
      "python_abi-3.9       | 4 KB      | ########## | 100% \n",
      "\n",
      "dlib-19.24.0         | 4.4 MB    |            |   0% \n",
      "dlib-19.24.0         | 4.4 MB    |            |   0% \n",
      "dlib-19.24.0         | 4.4 MB    | 8          |   8% \n",
      "dlib-19.24.0         | 4.4 MB    | #6         |  16% \n",
      "dlib-19.24.0         | 4.4 MB    | ##3        |  24% \n",
      "dlib-19.24.0         | 4.4 MB    | ###2       |  32% \n",
      "dlib-19.24.0         | 4.4 MB    | ####5      |  46% \n",
      "dlib-19.24.0         | 4.4 MB    | #####7     |  58% \n",
      "dlib-19.24.0         | 4.4 MB    | #######1   |  72% \n",
      "dlib-19.24.0         | 4.4 MB    | ########8  |  88% \n",
      "dlib-19.24.0         | 4.4 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b545a",
   "metadata": {},
   "source": [
    "### Libreria face_recognition\n",
    "\n",
    "face_recognition es una herramienta de Python basada en el reconocimiento facial que utiliza redes neuronales profundas para identificar y verificar rostros en imágenes y videos.Esta libreria permite realizar operaciones avanzadas de procesamiento de imágenes, como detección de rostros, reconocimiento de identidad y comparación de rostros. Su facilidad de uso y precisión hacen que face_recognition sea ampliamente utilizada en aplicaciones de seguridad, autenticación biométrica y análisis de datos visuales en tiempo real, simplificando la implementación de algoritmos complejos de visión artificial en proyectos de reconocimiento facial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ad0e35-4a0a-465c-a36d-14def37f9120",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Admin\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - face_recognition\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    face_recognition-1.3.0     |     pyhd3deb0d_2          17 KB  conda-forge\n",
      "    face_recognition_models-0.3.0|     pyh9f0ad1d_0        87.7 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        87.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  face_recognition   conda-forge/noarch::face_recognition-1.3.0-pyhd3deb0d_2\n",
      "  face_recognition_~ conda-forge/noarch::face_recognition_models-0.3.0-pyh9f0ad1d_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "face_recognition-1.3 | 17 KB     |            |   0% \n",
      "face_recognition-1.3 | 17 KB     | #########5 |  95% \n",
      "face_recognition-1.3 | 17 KB     | ########## | 100% \n",
      "\n",
      "face_recognition_mod | 87.7 MB   |            |   0% \n",
      "face_recognition_mod | 87.7 MB   |            |   0% \n",
      "face_recognition_mod | 87.7 MB   | 1          |   1% \n",
      "face_recognition_mod | 87.7 MB   | 2          |   2% \n",
      "face_recognition_mod | 87.7 MB   | 2          |   3% \n",
      "face_recognition_mod | 87.7 MB   | 4          |   4% \n",
      "face_recognition_mod | 87.7 MB   | 5          |   6% \n",
      "face_recognition_mod | 87.7 MB   | 8          |   8% \n",
      "face_recognition_mod | 87.7 MB   | 9          |  10% \n",
      "face_recognition_mod | 87.7 MB   | #2         |  12% \n",
      "face_recognition_mod | 87.7 MB   | #3         |  14% \n",
      "face_recognition_mod | 87.7 MB   | #6         |  16% \n",
      "face_recognition_mod | 87.7 MB   | #7         |  18% \n",
      "face_recognition_mod | 87.7 MB   | #9         |  20% \n",
      "face_recognition_mod | 87.7 MB   | ##1        |  22% \n",
      "face_recognition_mod | 87.7 MB   | ##3        |  23% \n",
      "face_recognition_mod | 87.7 MB   | ##5        |  26% \n",
      "face_recognition_mod | 87.7 MB   | ##7        |  28% \n",
      "face_recognition_mod | 87.7 MB   | ##9        |  29% \n",
      "face_recognition_mod | 87.7 MB   | ###1       |  31% \n",
      "face_recognition_mod | 87.7 MB   | ###2       |  33% \n",
      "face_recognition_mod | 87.7 MB   | ###4       |  35% \n",
      "face_recognition_mod | 87.7 MB   | ###6       |  36% \n",
      "face_recognition_mod | 87.7 MB   | ###8       |  38% \n",
      "face_recognition_mod | 87.7 MB   | ###9       |  40% \n",
      "face_recognition_mod | 87.7 MB   | ####1      |  42% \n",
      "face_recognition_mod | 87.7 MB   | ####4      |  44% \n",
      "face_recognition_mod | 87.7 MB   | ####5      |  46% \n",
      "face_recognition_mod | 87.7 MB   | ####7      |  48% \n",
      "face_recognition_mod | 87.7 MB   | ####9      |  50% \n",
      "face_recognition_mod | 87.7 MB   | #####1     |  51% \n",
      "face_recognition_mod | 87.7 MB   | #####3     |  54% \n",
      "face_recognition_mod | 87.7 MB   | #####5     |  55% \n",
      "face_recognition_mod | 87.7 MB   | #####7     |  57% \n",
      "face_recognition_mod | 87.7 MB   | #####9     |  60% \n",
      "face_recognition_mod | 87.7 MB   | ######1    |  62% \n",
      "face_recognition_mod | 87.7 MB   | ######3    |  63% \n",
      "face_recognition_mod | 87.7 MB   | ######5    |  65% \n",
      "face_recognition_mod | 87.7 MB   | ######7    |  67% \n",
      "face_recognition_mod | 87.7 MB   | ######9    |  69% \n",
      "face_recognition_mod | 87.7 MB   | #######1   |  71% \n",
      "face_recognition_mod | 87.7 MB   | #######3   |  73% \n",
      "face_recognition_mod | 87.7 MB   | #######5   |  75% \n",
      "face_recognition_mod | 87.7 MB   | #######7   |  77% \n",
      "face_recognition_mod | 87.7 MB   | #######9   |  79% \n",
      "face_recognition_mod | 87.7 MB   | ########1  |  81% \n",
      "face_recognition_mod | 87.7 MB   | ########3  |  83% \n",
      "face_recognition_mod | 87.7 MB   | ########4  |  85% \n",
      "face_recognition_mod | 87.7 MB   | ########6  |  87% \n",
      "face_recognition_mod | 87.7 MB   | ########8  |  89% \n",
      "face_recognition_mod | 87.7 MB   | #########  |  90% \n",
      "face_recognition_mod | 87.7 MB   | #########2 |  92% \n",
      "face_recognition_mod | 87.7 MB   | #########3 |  94% \n",
      "face_recognition_mod | 87.7 MB   | #########5 |  96% \n",
      "face_recognition_mod | 87.7 MB   | #########7 |  98% \n",
      "face_recognition_mod | 87.7 MB   | #########9 | 100% \n",
      "face_recognition_mod | 87.7 MB   | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conda install -c conda-forge face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb49d17e-8ee7-42ba-9995-c35647b7ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "#Para el procesamiento de imágenes y visión artificial en Python\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd406f18-be48-4ad3-b81c-6f4f0a96889a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Using cached imutils-0.5.4.tar.gz (17 kB)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py): started\n",
      "  Building wheel for imutils (setup.py): finished with status 'done'\n",
      "  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25872 sha256=17a7cb23174346c15ff17d2f92a107947d6da7b74d2772060ffa9e2441f50d30\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\4b\\a5\\2d\\4a070a801d3a3d93f033d3ee9728f470f514826e89952df3ea\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Libreria complementaria para trabajar con OpenCV, se usa para la rotación, el cambio de tamaño, la traslación y la detección de bordes en imágenes.\n",
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b462c85",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95dabc",
   "metadata": {},
   "source": [
    "##### Creación de imagenes sintéticas\n",
    "Para esta parte se utilizará imágenes sintéticas generadas a partir de una sola foto por alumno del 10mo ciclo. Esto nos ayudará a crear un conjunto de imágenes más amplio para entrenar nuestro modelo de reconocimiento facial. Además usaremos fotos de actores reconocidos de peliculas en estreno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc60448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería Keras (con ImageDataGenerator)\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta a la carpeta del dataset\n",
    "dataset_path = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de aumentación de datos\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,         # Rotación hasta 15 grados\n",
    "    width_shift_range=0.1,     # Desplazamiento horizontal\n",
    "    height_shift_range=0.1,    # Desplazamiento vertical\n",
    "    brightness_range=[0.8, 1.2],  # Variación de brillo\n",
    "    zoom_range=0.2             # Zoom\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar imágenes aumentadas para cada persona\n",
    "for person_name in os.listdir(dataset_path):\n",
    "    person_folder = os.path.join(dataset_path, person_name)\n",
    "    if os.path.isdir(person_folder):\n",
    "        # Cargar la imagen original de la persona\n",
    "        original_image_path = os.path.join(person_folder, f\"{person_name}_original.jpg\")\n",
    "        img = load_img(original_image_path)\n",
    "        x = img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)  # Ajustar la imagen para el generador\n",
    "\n",
    "        # Crear 20 imágenes aumentadas por cada imagen original\n",
    "        i = 0\n",
    "        for batch in datagen.flow(x, batch_size=1, save_to_dir=person_folder, save_prefix=\"aug\", save_format=\"jpg\"):\n",
    "            i += 1\n",
    "            if i >= 20:  # Ajusta el número de imágenes aumentadas si es necesario\n",
    "                break\n",
    "\n",
    "print(\"Dataset aumentado creado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b762f4-053b-445c-9dc2-263a1a68ae08",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OpenCV para codificación de los rostros\n",
    "\n",
    "Usaremos para poder reconocer rostros en imágenes y videos, primero necesito codificar (o cuantificar) los rostros en mi conjunto de entrenamiento. Es importante destacar que en este caso no se va entrenar una red neuronal desde cero: la red (en la libreria face_recognition) ya ha sido entrenada para generar embeddings de 128 dimensiones a partir de un conjunto de datos de aproximadamente 3 millones de imágenes.\n",
    "\n",
    "Obviamente se podría entrenar una red desde cero o incluso ajustar los pesos de un modelo existente, pero eso demandaria mucho tiempo.\n",
    "Además, necesitaría una gran cantidad de imágenes para entrenar la red desde cero. En su lugar, es más sencillo utilizar la red preentrenada y emplearla para construir embeddings de 128 dimensiones para cada una mis imagenes de mi conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5ee85",
   "metadata": {},
   "source": [
    "Para clasificar cada rostro, estoy usando un modelo llamado KNN (o K-Nearest Neighbors), que es una forma de aprendizaje automático que compara caras para encontrar las más parecidas.\n",
    "\n",
    "Así es como funciona: cuando el modelo ve un nuevo rostro, busca en su \"memoria\" (es decir, su conjunto de datos) los rostros más parecidos a ese nuevo rostro. Luego, hace un \"sistema de votos\" con esos rostros cercanos para decidir quién es. Imagina que los rostros cercanos “votan” por quién creen que es el nuevo rostro, y la opción con más votos es el resultado final.\n",
    "\n",
    "Este método de KNN es solo una forma de hacerlo. Existen otros modelos de machine learning que también podrían usarse para hacer la misma tarea de clasificación de rostros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebda177-9b2c-4c8d-a3a2-624f150b62bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\proyecto_final\\\\face'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importación de paquetes necesarios\n",
    "from imutils import paths\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'               # Ruta al directorio de entrada de rostros e imágenes\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'    # Ruta a la base de datos serializada de codificaciones faciales\n",
    "args['detection_method'] = 'cnn'                          # Modelo de detección facial a utilizar: el método CNN es más preciso pero lento.\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8057d03-48f8-4d42-afae-9a50dd0478b8",
   "metadata": {},
   "source": [
    "# Crear embeddings faciales\n",
    "\n",
    "Un embedding facial es como una \"huella digital\" especial para cada cara, hecha de números en lugar de líneas. Cuando la computadora ve una cara, convierte esa cara en un grupo de números únicos que representan cómo se ve esa persona.\n",
    "\n",
    "Esos números son como una “tarjeta de identificación” para cada rostro: si ve una cara similar, puede comparar sus \"huellas\" de números para ver si son parecidas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163e4376-1849-4c27-b1c4-496e2e165326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_facial_embeddings(args):\n",
    "    # obtener las rutas de las imágenes de entrada en nuestro conjunto de datos\n",
    "    print('[INFO] cuantificando rostros...')\n",
    "    imagePaths = list(paths.list_images(args['dataset']))\n",
    "    # inicializar la lista de codificaciones conocidas y nombres conocidos\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        print(i, imagePath)\n",
    "\n",
    "    # OpenCV ordena los canales de color en BGR, pero dlib en realidad espera RGB. El módulo face_recognition usa dlib, por lo que necesitamos cambiar el espacio de color y nombrar la nueva imagen rgb\n",
    "    ti = time.time()\n",
    "    print('[INFO] procesando imagen...')\n",
    "    # bucle sobre las rutas de las imágenes\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extraer el nombre de la persona de la ruta de la imagen\n",
    "        print('{}/{}'.format(i+1, len(imagePaths)), end=', ')\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        # cargar la imagen de entrada y convertirla de BGR (orden de OpenCV) al orden de dlib (RGB)\n",
    "        image = cv2.imread(imagePath)\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # detectar las coordenadas (x,y) de las cajas delimitadoras correspondientes a cada rostro en la imagen de entrada\n",
    "        boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "        # calcular el embedding facial para el rostro, es decir, convertir las cajas delimitadoras del rostro en una lista de 128 números\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        # bucle sobre las codificaciones\n",
    "        for encoding in encodings:\n",
    "            # agregar cada codificación + nombre a nuestro conjunto de nombres y codificaciones conocidos\n",
    "            knownEncodings.append(encoding)\n",
    "            knownNames.append(name)\n",
    "    print('¡Listo!')\n",
    "    print('Tiempo tomado: {:.1f} minutos'.format((time.time() - ti)/60))\n",
    "\n",
    "    # volcar los nombres y codificaciones al disco para un futuro uso\n",
    "    # encodings.pickle contiene los embeddings de rostros en 128 dimensiones para cada rostro en nuestro conjunto de datos\n",
    "    print('[INFO] serializando codificaciones...')\n",
    "    data = {'encodings': knownEncodings, 'names': knownNames}\n",
    "    f = open(args['encodings'], 'wb')\n",
    "    f.write(pickle.dumps(data))\n",
    "    f.close()\n",
    "    print('¡Listo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944f1c97-80f3-4c65-9d3a-2e3a55d4b728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "0 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(1).jpg\n",
      "1 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(10).jpg\n",
      "2 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(2).jpg\n",
      "3 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(3).jpg\n",
      "4 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(4).jpg\n",
      "5 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(5).jpg\n",
      "6 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(6).jpg\n",
      "7 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(7).jpg\n",
      "8 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(8).jpg\n",
      "9 d:\\proyecto_final\\face\\dataset\\brandon\\brandon(9).jpg\n",
      "10 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(1).jpg\n",
      "11 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(10).jpg\n",
      "12 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(2).jpg\n",
      "13 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(3).jpg\n",
      "14 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(4).jpg\n",
      "15 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(5).jpg\n",
      "16 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(6).jpg\n",
      "17 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(7).jpg\n",
      "18 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(8).jpg\n",
      "19 d:\\proyecto_final\\face\\dataset\\gabriel\\gabriel(9).jpg\n",
      "20 d:\\proyecto_final\\face\\dataset\\jan\\jan(1).jpg\n",
      "21 d:\\proyecto_final\\face\\dataset\\jan\\jan(2).jpg\n",
      "22 d:\\proyecto_final\\face\\dataset\\jan\\jan(3).jpg\n",
      "23 d:\\proyecto_final\\face\\dataset\\jan\\jan(4).jpg\n",
      "24 d:\\proyecto_final\\face\\dataset\\jan\\jan(5).jpg\n",
      "25 d:\\proyecto_final\\face\\dataset\\jan\\jan(6).jpg\n",
      "26 d:\\proyecto_final\\face\\dataset\\jan\\jan(7).jpg\n",
      "27 d:\\proyecto_final\\face\\dataset\\jan\\jan(8).jpg\n",
      "28 d:\\proyecto_final\\face\\dataset\\jan\\jan(9).jpg\n",
      "29 d:\\proyecto_final\\face\\dataset\\jeyson\\aug_0_4818.jpg\n",
      "30 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(1).jpg\n",
      "31 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(2).jpg\n",
      "32 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(3).jpg\n",
      "33 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(4).jpg\n",
      "34 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(5).jpg\n",
      "35 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(6).jpg\n",
      "36 d:\\proyecto_final\\face\\dataset\\jeyson\\jeyson(7).jpg\n",
      "37 d:\\proyecto_final\\face\\dataset\\jovani\\jovani(1).jpg\n",
      "38 d:\\proyecto_final\\face\\dataset\\juan\\juan(1).jpg\n",
      "39 d:\\proyecto_final\\face\\dataset\\juan\\juan(10).jpg\n",
      "40 d:\\proyecto_final\\face\\dataset\\juan\\juan(2).jpg\n",
      "41 d:\\proyecto_final\\face\\dataset\\juan\\juan(3).jpg\n",
      "42 d:\\proyecto_final\\face\\dataset\\juan\\juan(4).jpg\n",
      "43 d:\\proyecto_final\\face\\dataset\\juan\\juan(5).jpg\n",
      "44 d:\\proyecto_final\\face\\dataset\\juan\\juan(6).jpg\n",
      "45 d:\\proyecto_final\\face\\dataset\\juan\\juan(7).jpg\n",
      "46 d:\\proyecto_final\\face\\dataset\\juan\\juan(8).jpg\n",
      "47 d:\\proyecto_final\\face\\dataset\\juan\\juan(9).jpg\n",
      "48 d:\\proyecto_final\\face\\dataset\\noe\\noe(1).JPG\n",
      "49 d:\\proyecto_final\\face\\dataset\\noe\\noe(10).jpg\n",
      "50 d:\\proyecto_final\\face\\dataset\\noe\\noe(2).jpg\n",
      "51 d:\\proyecto_final\\face\\dataset\\noe\\noe(3).jpg\n",
      "52 d:\\proyecto_final\\face\\dataset\\noe\\noe(4).jpg\n",
      "53 d:\\proyecto_final\\face\\dataset\\noe\\noe(5).jpg\n",
      "54 d:\\proyecto_final\\face\\dataset\\noe\\noe(6).jpg\n",
      "55 d:\\proyecto_final\\face\\dataset\\noe\\noe(7).jpg\n",
      "56 d:\\proyecto_final\\face\\dataset\\noe\\noe(8).jpg\n",
      "57 d:\\proyecto_final\\face\\dataset\\noe\\noe(9).jpg\n",
      "58 d:\\proyecto_final\\face\\dataset\\saul\\saul(1).jpg\n",
      "59 d:\\proyecto_final\\face\\dataset\\saul\\saul(10).jpg\n",
      "60 d:\\proyecto_final\\face\\dataset\\saul\\saul(11).jpg\n",
      "61 d:\\proyecto_final\\face\\dataset\\saul\\saul(2).jpg\n",
      "62 d:\\proyecto_final\\face\\dataset\\saul\\saul(3).jpg\n",
      "63 d:\\proyecto_final\\face\\dataset\\saul\\saul(4).jpg\n",
      "64 d:\\proyecto_final\\face\\dataset\\saul\\saul(5).jpg\n",
      "65 d:\\proyecto_final\\face\\dataset\\saul\\saul(6).jpg\n",
      "66 d:\\proyecto_final\\face\\dataset\\saul\\saul(7).jpg\n",
      "67 d:\\proyecto_final\\face\\dataset\\saul\\saul(8).jpg\n",
      "68 d:\\proyecto_final\\face\\dataset\\saul\\saul(9).jpg\n",
      "[INFO] processing image...\n",
      "1/69, 2/69, 3/69, 4/69, 5/69, 6/69, 7/69, 8/69, 9/69, 10/69, 11/69, 12/69, 13/69, 14/69, 15/69, 16/69, 17/69, 18/69, 19/69, 20/69, 21/69, 22/69, 23/69, 24/69, 25/69, 26/69, 27/69, 28/69, 29/69, 30/69, 31/69, 32/69, 33/69, 34/69, 35/69, 36/69, 37/69, 38/69, 39/69, 40/69, 41/69, 42/69, 43/69, 44/69, 45/69, 46/69, 47/69, 48/69, 49/69, 50/69, 51/69, 52/69, 53/69, 54/69, 55/69, 56/69, 57/69, 58/69, 59/69, 60/69, 61/69, 62/69, 63/69, 64/69, 65/69, 66/69, 67/69, 68/69, 69/69, Done!\n",
      "Time taken: 14.8 minutes\n",
      "[INFO] serializing encodings...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Usando solo el CPU !!\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset'               # Ruta al directorio de entrada de rostros e imágenes\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'    # Ruta a la base de datos serializada de codificaciones faciales\n",
    "args['detection_method'] = 'cnn'                          # Modelo de detección facial a utilizar: el método CNN es más preciso pero lento.\n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b01cb-b0b3-4e92-8957-edaef3c9d1c1",
   "metadata": {},
   "source": [
    "# Reconocer rostros en imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a5448-ba20-4074-9632-a8e8140b9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'        # Ruta a la base de datos serializada de codificaciones faciales\n",
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'    # Ruta a la imagen de entrada\n",
    "args['detection_method'] = 'cnn'                              # Modelo de detección facial a utilizar: el método CNN es más preciso pero lento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2759d964-fb77-4b87-a4d8-b18fde1ee415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces(args):\n",
    "    ti = time.time()\n",
    "    # cargar los rostros y embeddings conocidos\n",
    "    print('[INFO] cargando codificaciones...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    # cargar la imagen de entrada y convertirla de BGR a RGB\n",
    "    image = cv2.imread(args['image'])\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # detectar las coordenadas (x,y) de las cajas delimitadoras correspondientes a cada rostro en la imagen de entrada, luego calcular los embeddings faciales para cada rostro\n",
    "    print('[INFO] reconociendo rostros...')\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    # inicializar la lista de nombres para cada rostro detectado\n",
    "    names = []\n",
    "    confidences = []\n",
    "\n",
    "    # bucle sobre los embeddings faciales\n",
    "    for encoding in encodings:\n",
    "        # intentar hacer coincidir cada rostro en la imagen de entrada con nuestras codificaciones conocidas, la función devuelve una lista de valores Verdadero/Falso, uno para cada codificación conocida\n",
    "        # calcular las distancias entre el encoding detectado y todas las codificaciones conocidas\n",
    "        distances = face_recognition.face_distance(data['encodings'], encoding)\n",
    "        # encontrar la coincidencia más cercana\n",
    "        best_match_index = np.argmin(distances)\n",
    "        best_distance = distances[best_match_index]\n",
    "        \n",
    "        # definir un umbral para considerar una coincidencia\n",
    "        threshold = 0.6  # Ajusta este valor según tus necesidades\n",
    "\n",
    "        # calcular la \"precisión\" como un porcentaje de confianza basado en la distancia\n",
    "        if best_distance < threshold:\n",
    "            accuracy = (1 - best_distance / threshold)\n",
    "            name = data['names'][best_match_index]\n",
    "        else:\n",
    "            accuracy = 0\n",
    "            name = 'Desconocido'\n",
    "\n",
    "        # actualizar la lista de nombres\n",
    "        names.append(name)\n",
    "        confidences.append(accuracy)\n",
    "\n",
    "    print([' '.join([e.title() for e in names])])\n",
    "    print('Tiempo tomado: {:.1f} segundos'.format(time.time() - ti))\n",
    "          \n",
    "   # Visualizar con cajas delimitadoras, nombres y porcentajes de confianza\n",
    "    for ((top, right, bottom, left), name, confidence) in zip(boxes, names, confidences):\n",
    "        # dibujar la caja delimitadora\n",
    "        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        display_text = f\"{name}: {confidence:.2f}%\" if confidence > 0 else name\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(image, display_text, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # mostrar el cuadro resultante, presiona 'q' para salir\n",
    "    window_text = args['image'].split(os.path.sep)[-1]\n",
    "    cv2.imshow(window_text, image)\n",
    "    while True:\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    # guardar la imagen de salida\n",
    "    cv2.imwrite(args['image'].rsplit('.', 1)[0] + '_output.jpg', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012914cf-68cb-4662-9919-358bb433781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognising faces...\n",
      "['Jeyson Noe Juan Jovani Brandon Brandon']\n",
      "Time taken: 491.6 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (1).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0d010fa-692e-4b90-957f-8ebaf2140729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognising faces...\n",
      "['Saul', 'Gabriel', 'Jeyson', 'Juan', 'Saul']\n",
      "Time taken: 121.3 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (2).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1db17e-5e93-4cd4-a7b0-330ccb731d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Unknown', 'Claire Dearing', 'Owen Grady', 'Unknown']\n",
      "Time taken: 25.0 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (3).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7e165b7-940d-4dec-9110-50c25533d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Alan Grant']\n",
      "Time taken: 20.3 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (4).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8442c50-1a8f-4c6c-8ede-bbcd42b450cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Ian Malcolm']\n",
      "Time taken: 154.1 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (5).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9d8846-fdfa-4b4c-91b1-a1b6ff56ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] recognizing faces...\n",
      "['Alan Grant', 'Ian Malcolm', 'Ellie Sattler']\n",
      "Time taken: 41.8 seconds\n"
     ]
    }
   ],
   "source": [
    "args['image'] = os.getcwd() + '\\\\image_test\\\\test (6).jpg'\n",
    "recognise_faces(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ba294-dacc-4351-a702-cd8eeac18d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f619512-3105-464b-9463-95a74a8dd3af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reconocer rostros en un archivo de video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d9f0988-0d05-425c-a8b2-a319d5aeab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paquetes necesarios \n",
    "import face_recognition  # Biblioteca principal para la detección y reconocimiento de rostros.\n",
    "import imutils  # Facilita operaciones comunes de procesamiento de imágenes, como redimensionar y rotar fotogramas de video, optimizando el procesamiento y haciendo que los fotogramas tengan un tamaño adecuado.\n",
    "import pickle  # Biblioteca utilizada para serializar y deserializar datos (guardar y cargar). \n",
    "import cv2  # Biblioteca OpenCV, fundamental para leer, procesar y mostrar cada fotograma del video, además de realizar conversiones de color y dibujar cuadros delimitadores alrededor de los rostros.\n",
    "import os  # Permite interactuar con el sistema de archivos, como manejar rutas y directorios.\n",
    "import time  # Utilizada para medir el tiempo de procesamiento.\n",
    "from collections import Counter  # Herramienta para contar la frecuencia de aparición de rostros reconocidos, ayudando a identificar qué rostros aparecen con más frecuencia en el video.\n",
    "\n",
    "\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings.pickle'              # ruta a la base de datos serializada de codificaciones faciales\n",
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'           # ruta al video de entrada\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_outputt.avi'   # ruta al video de salida\n",
    "args['display'] = 1                                                 # mostrar el cuadro de salida en pantalla: sí o no\n",
    "args['detection_method'] = 'hog'                                    # modelo de detección facial a utilizar: el método CNN es más preciso pero más lento. HOG es más rápido pero menos preciso.\n",
    "\n",
    "# Ahora usaremos hog ya que solo estoy usando mi cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffdcbd36-bb8d-4443-8925-0af0446f0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognise_faces_video(args):\n",
    "    ti = time.time()\n",
    "    # cargar los rostros y embeddings conocidos\n",
    "    print('[INFO] cargando codificaciones...')\n",
    "    data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "    # inicializar el puntero al archivo de video y el escritor de video\n",
    "    print('[INFO] procesando video...')\n",
    "    stream = cv2.VideoCapture(args['input'])\n",
    "    writer = None    # opcionalmente escribiendo cuadros de video procesados en el disco más tarde, por lo que inicializamos writer en None\n",
    "\n",
    "    # bucle sobre los cuadros del flujo de archivos de video\n",
    "    while True:\n",
    "        # obtener el siguiente cuadro\n",
    "        (grabbed, frame) = stream.read()\n",
    "        # si no se obtuvo el cuadro, hemos llegado al final del flujo\n",
    "        if not grabbed:\n",
    "            break\n",
    "        # convertir el cuadro de entrada de BGR a RGB y luego redimensionarlo a un ancho de 750px (para acelerar el procesamiento)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb = imutils.resize(frame, width=750)\n",
    "        r = frame.shape[1] / float(rgb.shape[1])\n",
    "        # detectar las coordenadas (x,y) de las cajas delimitadoras correspondientes a cada rostro en el cuadro de entrada, luego calcular los embeddings faciales para cada rostro\n",
    "        boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        names = []\n",
    "\n",
    "        # bucle sobre los embeddings faciales\n",
    "        for encoding in encodings:\n",
    "            # intentar hacer coincidir cada rostro en la imagen de entrada con nuestras codificaciones conocidas, la función devuelve una lista de valores Verdadero/Falso, uno para cada codificación conocida\n",
    "            # Internamente, la función compare_faces está calculando la distancia euclidiana entre el embedding candidato y todos los rostros en nuestras codificaciones conocidas\n",
    "            votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "            # verificar si se encontró una coincidencia\n",
    "            if True in votes:\n",
    "                # encontrar los nombres correspondientes de todos los rostros coincidentes (vote==True)\n",
    "                matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "                # determinar el nombre que ocurre con mayor frecuencia (nota: en el improbable caso de un empate, Python seleccionará la primera entrada en el diccionario)\n",
    "                name = Counter(matches).most_common()[0][0]\n",
    "            else:\n",
    "                name = 'Desconocido'\n",
    "            # actualizar la lista de nombres\n",
    "            names.append(name)\n",
    "\n",
    "        # visualizar con cajas delimitadoras y nombres etiquetados, recorrer los rostros reconocidos\n",
    "        for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "            # reescalar las coordenadas del rostro\n",
    "            top = int(top * r)\n",
    "            right = int(right * r)\n",
    "            bottom = int(bottom * r)\n",
    "            left = int(left * r)\n",
    "            # dibujar el nombre del rostro predicho en la imagen\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            y = top - 15 if top - 15 > 15 else top + 15\n",
    "            cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "        # si el escritor de video es None *Y* se proporciona la ruta de salida (para escribir el cuadro en el disco)\n",
    "        if writer is None and args['output'] is not None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')    # para usar el código de 4 caracteres “MJPG”\n",
    "            writer = cv2.VideoWriter(args['output'], fourcc, 24, (frame.shape[1], frame.shape[0]), True)    # ruta del archivo de salida, fourcc, objetivo de fotogramas por segundo y dimensiones del cuadro\n",
    "        # si el escritor no es None, escribir el cuadro con rostros reconocidos en el disco\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "\n",
    "        # verificar si se muestra el cuadro de salida en pantalla\n",
    "        if args['display'] == 1:\n",
    "            cv2.imshow('Video file', frame)\n",
    "            # si se presiona la tecla `q`, salir del bucle\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    # hacer algo de limpieza\n",
    "    cv2.destroyAllWindows()\n",
    "    stream.release()    # cerrar los punteros del archivo de video\n",
    "    # verificar si es necesario liberar el puntero del escritor de video\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print('Tiempo tomado: {:.1f} minutos'.format((time.time() - ti)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e487c837-e055-49d7-b5e8-8047633151cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] processing video...\n",
      "Time taken: 15.2 minutes\n"
     ]
    }
   ],
   "source": [
    "args['input'] = os.getcwd() + '\\\\video_test\\\\trailer.mp4'\n",
    "args['output'] = args['input'].rsplit('.', 1)[0] + '_outputt.avi'\n",
    "recognise_faces_video(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06614fc-4bec-494c-9e38-f87431bb6428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7865f9d-1954-4069-9fd4-a3c1f22942d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reconocer rostros en la cámara web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168616d1-78e8-4520-945b-8dfa40c063e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "from imutils import paths\n",
    "from imutils.video import VideoStream\n",
    "import face_recognition\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4160e3b4-e174-4674-afd3-a93dbfe4011d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] quantifying faces...\n",
      "0 d:\\proyecto_final\\face\\dataset_webcam\\james\\Screenshot_8.jpg\n",
      "1 d:\\proyecto_final\\face\\dataset_webcam\\Jhonatan\\Jhonatan.jpg\n",
      "[INFO] processing image...\n",
      "1/2, 2/2, Done!\n",
      "Time taken: 2.9 minutes\n",
      "[INFO] serializing encodings...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#create facial embeddings\n",
    "args = {}\n",
    "args['dataset'] = os.getcwd() + '\\\\dataset_webcam'               \n",
    "args['encodings'] = os.getcwd() + '\\\\encodings_webcam.pickle'   \n",
    "args['detection_method'] = 'cnn'                                 \n",
    "\n",
    "create_facial_embeddings(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0a275d5-438f-4bfa-9dfc-4b2de5e1e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encendido de la camara\n",
    "args = {}\n",
    "args['encodings'] = os.getcwd() + '\\\\encodings_webcam.pickle'    \n",
    "args['output'] = os.getcwd() + '\\\\webcam_test\\\\output.avi'       \n",
    "args['display'] = 1                                              \n",
    "args['detection_method'] = 'hog' \n",
    "\n",
    "#hog por mi cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d780b84-d762-4799-939b-7980bb221a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading encodings...\n",
      "[INFO] starting video stream...\n",
      "Done! \n",
      "Time taken: 0.7 minutes\n"
     ]
    }
   ],
   "source": [
    "ti = time.time()\n",
    "# cargar los rostros y embeddings conocidos\n",
    "print('[INFO] cargando codificaciones...')\n",
    "data = pickle.loads(open(args['encodings'], 'rb').read())\n",
    "# inicializar el flujo de video y el puntero al archivo de video de salida, luego permitir que el sensor de la cámara se caliente\n",
    "print('[INFO] iniciando flujo de video...')\n",
    "vs = VideoStream(src=0).start()    # usar VideoStream para acceder a la cámara web, usar src=1 para una segunda cámara web\n",
    "time.sleep(2.0)    # time.sleep con 2 segundos para calentar la cámara web\n",
    "writer = None    # opcionalmente escribiendo cuadros de video procesados en el disco más tarde, por lo que inicializamos writer en None\n",
    "\n",
    "# bucle sobre los cuadros del flujo de video\n",
    "while True:\n",
    "    # obtener un cuadro del flujo de video en paralelo\n",
    "    frame = vs.read()\n",
    "    # convertir el cuadro de entrada de BGR a RGB y luego redimensionarlo a un ancho de 750px (para acelerar el procesamiento)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    rgb = imutils.resize(frame, width=750)\n",
    "    r = frame.shape[1] / float(rgb.shape[1])\n",
    "    # detectar las coordenadas (x,y) de las cajas delimitadoras correspondientes a cada rostro en el cuadro de entrada, luego calcular los embeddings faciales para cada rostro\n",
    "    boxes = face_recognition.face_locations(rgb, model=args['detection_method'])\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    names = []\n",
    "    # bucle sobre los embeddings faciales\n",
    "    for encoding in encodings:\n",
    "        # intentar hacer coincidir cada rostro en la imagen de entrada con nuestras codificaciones conocidas, la función devuelve una lista de valores Verdadero/Falso, uno para cada codificación conocida\n",
    "        # Internamente, la función compare_faces está calculando la distancia euclidiana entre el embedding candidato y todos los rostros en nuestras codificaciones conocidas\n",
    "        votes = face_recognition.compare_faces(data['encodings'], encoding)\n",
    "        # verificar si se encontró una coincidencia\n",
    "        if True in votes:\n",
    "            # encontrar los nombres correspondientes de todos los rostros coincidentes (vote==True)\n",
    "            matches = [name for name, vote in list(zip(data['names'], votes)) if vote == True]  \n",
    "            # determinar el nombre que ocurre con mayor frecuencia (nota: en el improbable caso de un empate, Python seleccionará la primera entrada en el diccionario)\n",
    "            name = Counter(matches).most_common()[0][0]\n",
    "        else:\n",
    "            name = 'Desconocido'\n",
    "        # actualizar la lista de nombres\n",
    "        names.append(name)\n",
    "\n",
    "    # visualizar con cajas delimitadoras y nombres etiquetados, recorrer los rostros reconocidos\n",
    "    for ((top, right, bottom, left), name) in zip(boxes, names):\n",
    "        # reescalar las coordenadas del rostro\n",
    "        top = int(top * r)\n",
    "        right = int(right * r)\n",
    "        bottom = int(bottom * r)\n",
    "        left = int(left * r)\n",
    "        # dibujar el nombre del rostro predicho en la imagen\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        y = top - 15 if top - 15 > 15 else top + 15\n",
    "        cv2.putText(frame, name, (left, y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # si el escritor de video es None *Y* se proporciona la ruta de salida (para escribir el cuadro en el disco)\n",
    "    if writer is None and args['output'] is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')    # para usar el código de 4 caracteres “MJPG”\n",
    "        writer = cv2.VideoWriter(args['output'], fourcc, 20, (frame.shape[1], frame.shape[0]), True)    # ruta del archivo de salida, fourcc, objetivo de fotogramas por segundo y dimensiones del cuadro\n",
    "    # si el escritor no es None, escribir el cuadro con rostros reconocidos en el disco\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    # verificar si se muestra el cuadro de salida en pantalla\n",
    "    if args['display'] == 1:\n",
    "        cv2.imshow('Webcam', frame)\n",
    "        # si se presiona la tecla `q`, salir del bucle\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "# hacer algo de limpieza\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()\n",
    "# verificar si es necesario liberar el puntero del escritor de video\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "print('¡Listo! \\nTiempo tomado: {:.1f} minutos'.format((time.time() - ti)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeb733-f05c-4088-8f1e-a5bc6b1ab42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776518d5-181c-4267-95c5-06c43b2c216e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
